{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce40b936",
   "metadata": {},
   "source": [
    "<!-- same as self attention just mask half of the matrix -->\n",
    "MaskedAttention(Q,K,V)=softmax(​QKT​/squt(dk)+M)V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2075280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e012d1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 5])\n",
      "tensor([[-0.4029,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.0494,  0.0263,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.1833,  0.4050, -0.1319,    -inf,    -inf,    -inf],\n",
      "        [-0.2482, -0.3558, -0.1158, -0.4465,    -inf,    -inf],\n",
      "        [-0.1443, -0.0889, -0.3397,  0.0169, -0.6495,    -inf],\n",
      "        [ 0.2874,  0.3747, -0.1849,  0.5616, -0.6385, -0.6354]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[-0.5108,  0.3980,  0.1469,  0.6822,  0.3239],\n",
      "        [-0.2769,  0.1798,  0.2572,  0.3694, -0.0795],\n",
      "        [-0.2716,  0.1757,  0.0712,  0.2454, -0.1730],\n",
      "        [-0.1953,  0.1891,  0.1777,  0.2617, -0.1189],\n",
      "        [-0.2008,  0.2925,  0.2801,  0.3167, -0.0222],\n",
      "        [-0.1014,  0.3059,  0.3471,  0.3449,  0.0103]], grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\goura\\AppData\\Local\\Temp\\ipykernel_23256\\3155872679.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  b = torch.tensor(a, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class MaskAttention(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.embdim = emb_dim\n",
    "        self.k = nn.Linear(emb_dim,emb_dim)\n",
    "        self.q = nn.Linear(emb_dim,emb_dim)\n",
    "        self.v = nn.Linear(emb_dim,emb_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        T, C = x.shape\n",
    "        k = self.k(x)\n",
    "        q = self.q(x)\n",
    "        v = self.v(x)\n",
    "        print(k.shape)\n",
    "        attention = (q@k.transpose(-1,-2))/(self.embdim**0.5)\n",
    "        mask = torch.tril(torch.ones(T, T, device=x.device))\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf'))\n",
    "        mask = mask.masked_fill(mask == 1, 0.0)\n",
    "        attention = attention + mask\n",
    "        print(attention)\n",
    "        attention = F.softmax(attention,dim=-1)\n",
    "        out = attention @ v\n",
    "        print(out)\n",
    "\n",
    "        # result = v@F.Softmax(attention)\n",
    "\n",
    "a = torch.randn(6, 5)\n",
    "b = torch.tensor(a, requires_grad=True)\n",
    "c1 = MaskAttention(emb_dim=b.shape[-1])\n",
    "c1(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205690bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
