{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cab9a96e",
   "metadata": {},
   "source": [
    "Concept 1 :-  Generating 3 different matrix with only one single matrix data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1fac7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffe17d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4251,  0.4382,  0.3007,  0.3169,  0.8361],\n",
      "        [ 0.3076,  0.2251,  1.2821,  0.6387, -0.6614],\n",
      "        [ 0.8434,  1.0309, -0.1368, -0.3322,  0.0446],\n",
      "        [ 0.2804, -0.0390,  1.0247,  0.7031, -1.3931],\n",
      "        [ 0.0178, -0.0484,  0.3412,  1.0037,  0.1106],\n",
      "        [ 0.3061,  0.5738, -0.2052,  0.1140,  0.5601]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.2106,  0.5981,  0.9899, -0.5624, -0.8608],\n",
      "        [ 0.0428,  0.2853, -0.0715, -0.4013,  0.8694],\n",
      "        [-0.7615, -0.4977, -1.0247, -0.1475,  0.6609],\n",
      "        [-0.1677,  0.9089, -0.2905, -1.8193, -0.0843],\n",
      "        [ 1.0597,  0.6157,  0.1083, -0.2329,  0.1097],\n",
      "        [ 0.1666,  0.0808, -0.1704, -0.0370,  0.0958]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6743, -0.1719,  1.1816, -0.8160, -1.2085],\n",
      "        [-0.5060,  0.1662,  0.7525,  0.1128, -1.2665],\n",
      "        [ 1.1471,  1.1121, -0.6207,  1.2654, -0.4611],\n",
      "        [ 0.9681,  0.4220,  0.9354,  0.6982, -0.8454],\n",
      "        [-1.0337, -0.7516,  0.2145, -0.3838, -0.3424],\n",
      "        [-0.0488,  0.1612, -0.1214,  0.2143, -0.4001]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\goura\\AppData\\Local\\Temp\\ipykernel_22096\\3350324195.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  b = torch.tensor(a, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class Concept1(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.embdim = emb_dim\n",
    "        self.k = nn.Linear(emb_dim,emb_dim)\n",
    "        self.q = nn.Linear(emb_dim,emb_dim)\n",
    "        self.v = nn.Linear(emb_dim,emb_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        k = self.k(x)\n",
    "        q = self.q(x)\n",
    "        v = self.v(x)\n",
    "        print(k)\n",
    "        print(q)\n",
    "        print(v)\n",
    "\n",
    "\n",
    "a = torch.randn(6, 5)\n",
    "b = torch.tensor(a, requires_grad=True)\n",
    "\n",
    "c1 = Concept1(emb_dim=b.shape[-1])\n",
    "c1(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320b4445",
   "metadata": {},
   "source": [
    "concept :-\n",
    "matmul(v,softmax(matrixmul(k,qT)/scaler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67da722b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.8464e-01,  2.9192e-01, -7.2281e-02, -3.6686e-02, -1.8612e-03,\n",
      "          4.9618e-02,  4.8497e-01,  1.5501e-01,  3.7335e-01,  7.6523e-02,\n",
      "         -3.2601e-01, -3.3364e-01, -1.9043e-01, -7.3444e-02,  3.0483e-01,\n",
      "         -8.9185e-02,  5.3117e-02,  1.6729e-01,  2.4238e-01, -3.4038e-01,\n",
      "          1.6369e-01,  2.3353e-02, -1.1501e-01, -1.4158e-01, -2.2434e-01,\n",
      "          3.7792e-01,  8.1843e-02,  8.5872e-03,  9.4207e-02,  1.3596e-02,\n",
      "          2.8717e-01,  3.1241e-01, -3.7534e-01,  2.2513e-01,  4.1313e-02,\n",
      "          3.7895e-01,  2.1481e-01,  5.3027e-01,  2.0858e-01,  1.2127e-01,\n",
      "          1.4616e-01,  7.5241e-02, -2.3973e-01, -2.4743e-01, -3.1431e-01,\n",
      "          2.5668e-01, -1.6474e-01, -4.7860e-01,  1.9408e-01, -5.9220e-02,\n",
      "         -2.3748e-01, -1.9110e-01, -2.4003e-01, -1.4588e-01, -2.7533e-01,\n",
      "          3.8663e-02, -3.4679e-02,  1.3224e-01, -8.4317e-02,  4.2258e-01,\n",
      "         -4.3606e-01, -9.6355e-02, -1.0799e-02, -1.7911e-02,  3.2445e-01,\n",
      "         -1.4370e-01,  4.6965e-02, -2.4134e-01, -2.3244e-02, -2.4443e-01,\n",
      "         -1.3524e-01, -1.6521e-01,  5.1476e-01,  2.2288e-02, -1.2544e-01,\n",
      "         -1.5481e-01,  3.6594e-02,  1.9262e-01, -2.0520e-01,  9.4359e-02,\n",
      "          4.8097e-01,  1.9962e-01, -1.3982e-01, -1.0249e-01,  5.1467e-02,\n",
      "         -1.6922e-01, -1.0451e-01, -2.8601e-01, -4.9715e-02,  1.5427e-01,\n",
      "          1.8310e-02, -4.4199e-01,  2.9940e-01,  1.6149e-01,  1.2776e-01,\n",
      "         -3.6805e-01,  4.5272e-01, -1.6753e-01, -1.2382e-01, -3.1251e-01],\n",
      "        [-2.5639e-01,  3.9072e-01, -1.3378e-01,  2.1380e-02, -1.0980e-01,\n",
      "          1.4654e-02,  5.5074e-01,  2.7200e-01,  5.4458e-01,  1.8961e-01,\n",
      "         -3.8789e-01, -4.1921e-01, -1.5337e-01, -5.3159e-02,  3.7127e-01,\n",
      "         -7.3483e-02,  1.0689e-02,  8.7286e-02,  2.1823e-01, -4.7776e-01,\n",
      "          3.2833e-01,  4.7827e-03, -2.4941e-01,  7.5495e-02, -2.4635e-01,\n",
      "          3.6476e-01,  2.5120e-01, -7.7330e-02,  2.5984e-01,  1.6782e-02,\n",
      "          2.5566e-01,  4.5980e-01, -4.6888e-01,  1.3739e-01,  3.5809e-02,\n",
      "          4.8598e-01,  1.1789e-01,  3.4937e-01,  3.0447e-01,  8.7200e-02,\n",
      "          2.2295e-01,  1.5471e-01, -3.5656e-01, -2.5661e-01, -2.3965e-01,\n",
      "          2.2592e-01, -1.0080e-01, -4.4305e-01,  1.5474e-01, -1.0459e-01,\n",
      "         -1.7572e-01, -1.1861e-01, -2.8170e-01, -6.6626e-02, -3.5347e-01,\n",
      "          2.1179e-01, -9.4425e-03,  4.0843e-02,  1.8283e-02,  3.7129e-01,\n",
      "         -4.4714e-01, -3.0366e-02,  1.0609e-01, -3.2449e-02,  2.6686e-01,\n",
      "         -7.5528e-02,  1.5692e-01, -9.1067e-02,  6.7452e-02, -1.8932e-01,\n",
      "         -1.5447e-01, -9.2730e-02,  6.1249e-01,  7.9403e-02, -1.0088e-01,\n",
      "         -1.6957e-01,  1.5232e-01,  2.1549e-01, -2.3201e-01,  4.8655e-02,\n",
      "          5.9419e-01,  3.1272e-01, -1.5586e-01, -1.3693e-01,  8.2211e-02,\n",
      "         -5.5743e-02, -1.4200e-01, -2.4493e-01, -1.2786e-01,  8.0843e-02,\n",
      "          1.0082e-01, -4.4863e-01,  2.9440e-01,  1.2259e-01, -1.6944e-02,\n",
      "         -4.9330e-01,  4.3171e-01, -2.1511e-01, -2.0469e-01, -2.6992e-01],\n",
      "        [-3.1157e-01,  4.5156e-01, -3.6044e-01,  4.4446e-02, -2.7430e-01,\n",
      "         -2.5890e-02,  3.8982e-01,  2.9705e-01,  5.4714e-01, -5.0765e-02,\n",
      "         -2.3753e-01, -4.2493e-01,  1.2904e-01, -5.0113e-02,  1.7413e-01,\n",
      "          2.0666e-01,  2.7568e-01,  2.3618e-01,  2.4382e-01, -4.4323e-01,\n",
      "          1.3456e-02,  1.3927e-02, -2.6877e-02,  1.9217e-01, -5.1374e-01,\n",
      "          3.5222e-01,  2.5101e-01, -4.5330e-02,  2.5556e-01,  1.8898e-01,\n",
      "          1.2312e-01,  3.6041e-01, -4.7976e-01,  3.2120e-01, -1.9060e-01,\n",
      "          4.5483e-01,  1.0231e-01,  4.6929e-01,  4.0153e-01,  6.3770e-04,\n",
      "         -1.5272e-01,  2.1001e-01,  3.9680e-02, -2.5237e-01, -2.7928e-01,\n",
      "         -5.0910e-02, -1.4994e-01, -2.2248e-01,  9.8847e-02, -1.1740e-01,\n",
      "         -3.9459e-01, -4.2550e-01, -2.6348e-01, -1.3049e-01, -4.5712e-01,\n",
      "          4.2741e-01, -2.5460e-01, -1.3856e-01,  1.4978e-01,  8.7088e-02,\n",
      "         -4.9915e-01,  2.6837e-01,  1.2670e-01, -1.1240e-01,  2.8508e-01,\n",
      "         -2.0038e-01,  2.7144e-01, -1.6253e-01,  1.3595e-01, -1.8188e-01,\n",
      "         -2.9693e-01, -3.3717e-01,  3.6156e-01,  3.4949e-01, -2.4972e-01,\n",
      "          2.9147e-02, -1.8187e-01, -1.1236e-02,  1.3021e-01, -1.7527e-01,\n",
      "          7.7837e-01,  2.3822e-01, -1.2816e-02, -1.9477e-01,  1.8233e-01,\n",
      "          4.5353e-02, -4.6421e-02, -3.8044e-01, -6.4353e-02, -7.8849e-02,\n",
      "         -7.2058e-03, -3.8652e-01,  6.9173e-02,  1.5234e-01, -7.4839e-02,\n",
      "         -4.0795e-01,  4.0136e-01, -1.0189e-01, -3.3574e-01, -3.5160e-01],\n",
      "        [-2.6839e-01,  2.8740e-01, -8.1172e-02, -6.6866e-02, -6.6502e-02,\n",
      "          5.6438e-02,  2.9211e-01,  9.6301e-02,  2.5932e-01, -4.5254e-02,\n",
      "         -1.6901e-01, -1.9572e-01, -6.1264e-02, -1.4326e-01,  1.8987e-01,\n",
      "         -9.3525e-02,  2.6955e-01,  1.0789e-01,  3.6642e-01, -2.5197e-01,\n",
      "         -2.4383e-02,  2.6722e-02, -6.0850e-02, -1.1707e-01, -2.9118e-01,\n",
      "          4.5637e-01,  5.5212e-02,  6.7928e-02,  2.0728e-01,  4.9061e-02,\n",
      "          3.0812e-01,  2.8656e-01, -4.3136e-01,  2.8061e-01, -5.1197e-02,\n",
      "          4.3150e-01,  4.0204e-01,  4.5345e-01,  3.5863e-01,  1.1994e-01,\n",
      "         -3.4815e-02,  1.6948e-01,  1.3971e-02, -8.4790e-02, -3.9987e-01,\n",
      "          1.7693e-01, -7.8754e-02, -4.7462e-01,  2.3443e-01, -1.2171e-01,\n",
      "         -3.8654e-01, -3.1961e-01, -1.5611e-01, -2.5172e-01, -2.9307e-01,\n",
      "          3.0925e-02, -1.1178e-01,  5.0515e-02, -1.3805e-01,  3.5563e-01,\n",
      "         -4.7463e-01,  3.1400e-02,  1.1917e-01, -6.7743e-02,  1.7528e-01,\n",
      "         -1.3485e-01, -1.6957e-03, -4.5290e-01,  3.7110e-02, -1.9509e-01,\n",
      "         -2.0101e-01, -3.1670e-01,  3.9528e-01,  1.6770e-01, -1.9583e-01,\n",
      "         -1.9647e-02, -1.5559e-01,  1.7535e-01, -6.5497e-02, -5.6154e-02,\n",
      "          3.7681e-01,  1.1554e-01, -2.2641e-01, -4.4056e-02,  3.1446e-02,\n",
      "         -7.9285e-02, -1.4119e-01, -3.7198e-01,  2.4902e-02,  5.0391e-02,\n",
      "         -4.6627e-02, -3.8361e-01,  2.1610e-01,  1.8602e-01,  1.4442e-01,\n",
      "         -4.0574e-01,  4.0547e-01, -1.6918e-01, -1.1310e-01, -3.4591e-01],\n",
      "        [-2.5695e-01,  3.8158e-01, -8.8728e-02, -1.9308e-03, -1.2649e-01,\n",
      "          3.0816e-02,  4.5253e-01,  2.1551e-01,  4.3771e-01,  9.4165e-02,\n",
      "         -3.0775e-01, -3.4119e-01, -1.1138e-01, -1.0479e-01,  3.0030e-01,\n",
      "         -3.2539e-02,  1.6824e-01,  9.8257e-02,  2.8459e-01, -4.2144e-01,\n",
      "          1.7341e-01,  1.5514e-02, -1.6818e-01,  1.2649e-02, -2.8181e-01,\n",
      "          4.0840e-01,  1.9525e-01, -3.4611e-02,  2.4895e-01,  1.3922e-02,\n",
      "          2.6157e-01,  4.1376e-01, -4.9596e-01,  2.0584e-01, -1.1180e-03,\n",
      "          4.7785e-01,  2.6075e-01,  3.5800e-01,  3.6310e-01,  1.1599e-01,\n",
      "          8.2270e-02,  1.6857e-01, -1.8083e-01, -1.7260e-01, -3.2169e-01,\n",
      "          1.8474e-01, -7.9457e-02, -4.3096e-01,  1.8607e-01, -1.1815e-01,\n",
      "         -2.6850e-01, -2.2593e-01, -2.2943e-01, -1.3658e-01, -3.4721e-01,\n",
      "          1.8350e-01, -7.3752e-02,  3.3661e-02, -2.2666e-02,  3.5766e-01,\n",
      "         -4.8625e-01,  1.7874e-03,  1.3583e-01, -6.2118e-02,  2.1617e-01,\n",
      "         -9.0547e-02,  1.0634e-01, -2.0583e-01,  9.3071e-02, -1.8825e-01,\n",
      "         -2.0120e-01, -1.8913e-01,  5.1416e-01,  1.3513e-01, -1.4819e-01,\n",
      "         -8.6657e-02,  3.4387e-02,  1.8405e-01, -1.2153e-01,  3.8875e-03,\n",
      "          5.5238e-01,  2.5840e-01, -1.6184e-01, -1.3529e-01,  5.9680e-02,\n",
      "         -4.2917e-02, -1.4378e-01, -3.3773e-01, -7.9880e-02,  5.8792e-02,\n",
      "          2.9881e-02, -4.4796e-01,  2.2931e-01,  1.2799e-01,  2.7204e-02,\n",
      "         -4.5818e-01,  3.8251e-01, -1.8322e-01, -1.8887e-01, -3.0267e-01],\n",
      "        [-2.7501e-01,  3.8701e-01,  3.5234e-03, -2.7637e-02, -6.1249e-02,\n",
      "          9.5516e-02,  3.4603e-01,  1.5742e-01,  2.6129e-01,  4.0028e-03,\n",
      "         -2.7750e-01, -3.0425e-01, -1.8290e-01, -1.5844e-01,  2.7767e-01,\n",
      "         -4.1147e-03,  2.5894e-01,  1.1935e-01,  3.3045e-01, -3.4390e-01,\n",
      "          7.0390e-02,  2.0972e-03, -5.8960e-02, -9.2326e-02, -2.4849e-01,\n",
      "          4.9800e-01,  1.0121e-01, -2.6310e-02,  1.9118e-01, -1.3422e-03,\n",
      "          3.3177e-01,  3.4701e-01, -5.3481e-01,  2.2964e-01,  1.6283e-02,\n",
      "          4.7592e-01,  3.8283e-01,  3.9548e-01,  4.0789e-01,  2.0548e-01,\n",
      "         -1.1074e-02,  1.3142e-01, -8.1856e-02, -1.0873e-01, -4.0663e-01,\n",
      "          2.0929e-01, -8.1329e-02, -4.5200e-01,  2.0865e-01, -9.1225e-02,\n",
      "         -3.4653e-01, -3.3062e-01, -1.6247e-01, -2.0533e-01, -3.0488e-01,\n",
      "          1.1305e-01, -8.5060e-02,  1.1801e-01, -7.5104e-02,  4.2191e-01,\n",
      "         -5.3314e-01, -8.7091e-02,  1.0511e-01, -7.5295e-02,  1.9506e-01,\n",
      "         -8.3730e-02, -6.2564e-03, -3.0290e-01,  1.1338e-01, -2.2580e-01,\n",
      "         -2.2540e-01, -2.6504e-01,  4.5567e-01,  1.7097e-01, -1.9792e-01,\n",
      "         -3.2470e-02, -1.1060e-03,  1.8021e-01, -5.7962e-02,  2.2446e-02,\n",
      "          4.9469e-01,  1.7124e-01, -1.5847e-01, -1.3871e-01,  2.9951e-04,\n",
      "         -8.4450e-02, -1.4703e-01, -4.3091e-01, -1.0062e-02,  1.1893e-01,\n",
      "         -3.9361e-02, -4.9864e-01,  2.1001e-01,  1.2086e-01,  9.5756e-02,\n",
      "         -4.0867e-01,  3.7200e-01, -2.0962e-01, -1.7668e-01, -3.0083e-01]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "tensor([[0.1435, 0.1884, 0.0905, 0.1356, 0.2070, 0.2351],\n",
      "        [0.1285, 0.1468, 0.1025, 0.2165, 0.2608, 0.1449],\n",
      "        [0.2127, 0.1153, 0.2910, 0.1069, 0.1820, 0.0921],\n",
      "        [0.2878, 0.1630, 0.1100, 0.1028, 0.1254, 0.2109],\n",
      "        [0.1905, 0.1680, 0.1227, 0.1832, 0.1858, 0.1498],\n",
      "        [0.2230, 0.2352, 0.1105, 0.1383, 0.1309, 0.1621]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\goura\\AppData\\Local\\Temp\\ipykernel_22096\\2283042261.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  b = torch.tensor(a, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.embdim = emb_dim\n",
    "        self.k = nn.Linear(emb_dim,emb_dim)\n",
    "        self.q = nn.Linear(emb_dim,emb_dim)\n",
    "        self.v = nn.Linear(emb_dim,emb_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        k = self.k(x)\n",
    "        q = self.q(x)\n",
    "        v = self.v(x)\n",
    "        scaler = self.embdim\n",
    "        s1 = q @ k.transpose(-1,-2)/(scaler**0.5)\n",
    "        att = F.softmax(s1,dim=-1)\n",
    "        out = att @ v\n",
    "        return out, att\n",
    "\n",
    "a = torch.randn(6, 5)\n",
    "b = torch.tensor(a, requires_grad=True)\n",
    "\n",
    "attention = SelfAttention(emb_dim=b.shape[-1])\n",
    "out , att = attention(b)\n",
    "print(out)\n",
    "print(att)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d2a1c6",
   "metadata": {},
   "source": [
    "there can my many context of single sentance multihead allow to extract more than one context -> n head = n context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c375248d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
