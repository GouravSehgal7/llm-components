{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cab9a96e",
   "metadata": {},
   "source": [
    "Concept 1 :-  Generating 3 different matrix with only one single matrix data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1fac7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffe17d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2205, -0.5107, -0.5776,  1.0154, -0.3387],\n",
      "        [ 0.8051, -0.1997, -0.1864,  0.2760,  0.7929],\n",
      "        [-0.8045,  0.1211, -0.4704,  0.7659, -0.1839],\n",
      "        [ 0.3040, -0.7501, -0.5700,  0.7559,  0.4723],\n",
      "        [ 0.2932, -0.6044,  0.1543,  0.0408,  0.6389],\n",
      "        [-0.3585, -0.7708, -0.0678,  0.3714,  0.2312]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2144, -0.2434,  1.0975,  0.2637, -1.3388],\n",
      "        [ 1.0139, -0.6744, -0.2186, -1.0284, -0.1813],\n",
      "        [-0.7618,  0.1986,  0.8440, -0.4387,  0.1197],\n",
      "        [-0.2579, -0.4177,  0.2553,  0.0783, -0.9059],\n",
      "        [ 0.5311, -0.5970,  0.1204, -0.2205, -0.8894],\n",
      "        [-0.4706, -0.2649,  0.6005,  0.4256, -1.1617]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.2427,  0.6524,  0.3455,  0.0935, -1.1860],\n",
      "        [-0.2993, -1.3685, -0.0400,  0.5771, -0.5586],\n",
      "        [ 0.7445,  0.5836, -0.7850, -0.6655,  0.1324],\n",
      "        [ 0.0342, -0.5207,  0.4102,  0.2069, -0.6501],\n",
      "        [-0.2358, -0.8353,  0.1298,  0.4162, -0.7673],\n",
      "        [ 0.1983,  0.0453,  0.1522, -0.0392, -0.6777]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\goura\\AppData\\Local\\Temp\\ipykernel_18064\\3350324195.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  b = torch.tensor(a, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class Concept1(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.embdim = emb_dim\n",
    "        self.k = nn.Linear(emb_dim,emb_dim)\n",
    "        self.q = nn.Linear(emb_dim,emb_dim)\n",
    "        self.v = nn.Linear(emb_dim,emb_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        k = self.k(x)\n",
    "        q = self.q(x)\n",
    "        v = self.v(x)\n",
    "        print(k)\n",
    "        print(q)\n",
    "        print(v)\n",
    "\n",
    "a = torch.randn(6, 5)\n",
    "b = torch.tensor(a, requires_grad=True)\n",
    "\n",
    "c1 = Concept1(emb_dim=b.shape[-1])\n",
    "c1(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a47c3b",
   "metadata": {},
   "source": [
    "self attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320b4445",
   "metadata": {},
   "source": [
    "concept :-\n",
    "matmul(v,softmax(matrixmul(k,qT)/scaler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67da722b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\goura\\AppData\\Local\\Temp\\ipykernel_18064\\3045246291.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  b = torch.tensor(a, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.embdim = emb_dim\n",
    "        self.k = nn.Linear(emb_dim,emb_dim)\n",
    "        self.q = nn.Linear(emb_dim,emb_dim)\n",
    "        self.v = nn.Linear(emb_dim,emb_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        k = self.k(x)\n",
    "        q = self.q(x)\n",
    "        v = self.v(x)\n",
    "        print(k.shape)\n",
    "        scaler = self.embdim\n",
    "        s1 = q @ k.transpose(-1,-2)/(scaler**0.5)\n",
    "        att = F.softmax(s1,dim=-1)\n",
    "        out = att @ v\n",
    "        return out, att\n",
    "\n",
    "a = torch.randn(2, 6, 8)\n",
    "b = torch.tensor(a, requires_grad=True)\n",
    "B, T, E = b.shape\n",
    "attention = SelfAttention(emb_dim=b.shape[-1])\n",
    "out , att = attention(b)\n",
    "# print(out)\n",
    "# print(att)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99590e0",
   "metadata": {},
   "source": [
    "multihead attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beaf86a",
   "metadata": {},
   "source": [
    "Concept:-\n",
    "let say we have embedding dim as (2,5,8) then our responce should also contain 5 words as rows and 8 dim embedding of each word\n",
    "concept is we will adjust of the initial \n",
    "        self.k = nn.Linear(emb_dim,emb_dim)\n",
    "        self.q = nn.Linear(emb_dim,emb_dim)\n",
    "        self.v = nn.Linear(emb_dim,emb_dim)\n",
    "        in such a way that in the end of multihead attention process we satisfy the above condition\n",
    "cases\n",
    "1) (B, T, E) → (B, T, H, D)\n",
    "2) transpose (B, T, H, D) → (B, H, T, D) this will allow us to process all head result together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c375248d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1677,  0.2940,  0.4168, -0.0701],\n",
       "         [-0.1945,  0.2490,  0.4724, -0.0158],\n",
       "         [-0.1953,  0.3551,  0.3990, -0.0788],\n",
       "         [-0.1599,  0.2814,  0.3987, -0.0905]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Multihead1(nn.Module):\n",
    "    def __init__(self,emb_dim,n_heads):\n",
    "        super().__init__()\n",
    "        assert emb_dim % n_heads == 0, \"Embedding must divide heads\"\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = emb_dim // n_heads\n",
    "        self.k = nn.Linear(emb_dim,emb_dim)\n",
    "        self.q = nn.Linear(emb_dim,emb_dim)\n",
    "        self.v = nn.Linear(emb_dim,emb_dim)\n",
    "        self.W_o = nn.Linear(emb_dim, emb_dim)\n",
    "    def forward(self,x):\n",
    "        B, T, E = x.shape\n",
    "        q = self.k(x)\n",
    "        k = self.q(x)\n",
    "        v = self.v(x)\n",
    "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        att = q @ k.transpose(-2, -1)\n",
    "        att = att / (self.head_dim ** 0.5)\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        context = att @ v\n",
    "        context = context.transpose(1, 2)\n",
    "        print(context.shape)\n",
    "        context = context.reshape(B, T, E)\n",
    "        print(context.shape)\n",
    "        out = self.W_o(context)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,emb_d,n_head):\n",
    "        super().__init__()\n",
    "        assert emb_d%n_head == 0 , \"Embedding must divide head\"\n",
    "        self.nhead = n_head\n",
    "        self.dhead = emb_d//n_head\n",
    "        self.wk = nn.Linear(emb_d,emb_d*n_head)\n",
    "        self.wq = nn.Linear(emb_d,emb_d*n_head)\n",
    "        self.wv = nn.Linear(emb_d,emb_d*n_head)\n",
    "        self.wo = nn.Linear(emb_d*n_head,emb_d)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        B,T,E = x.shape\n",
    "        # (2,4,8) -> (2,4,32) -> (2,4,4,8)\n",
    "        k = self.wk(x).view(B,T,self.nhead,E).transpose(1,2)\n",
    "        q = self.wq(x).view(B,T,self.nhead,E).transpose(1,2)\n",
    "        v = self.wv(x).view(B,T,self.nhead,E).transpose(1,2)\n",
    "        att = q@k.transpose(-2,-1)\n",
    "        att = att / (self.dhead ** 0.5)\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        context = att @ v\n",
    "        context = context.transpose(1, 2)\n",
    "        # (2,4,4,8)->(2,4,32)\n",
    "        context = context.reshape(B, T, E*self.nhead)\n",
    "        print(context.shape)\n",
    "        # (2,4,32)->(2,4,8)\n",
    "        out = self.wo(context)\n",
    "        return out\n",
    "    \n",
    "a = torch.randn(1,4,4)\n",
    "m =MultiHeadAttention(a.shape[-1],4)\n",
    "m(a)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076cba3b",
   "metadata": {},
   "source": [
    "cross attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858eae78",
   "metadata": {},
   "source": [
    "positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7b1ebeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\goura\\AppData\\Local\\Temp\\ipykernel_18064\\33762896.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  b = torch.tensor(a, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def function(pos,i,d):\n",
    "    if(i%2==0):\n",
    "        return math.sin(pos/10000**(2*i/d))\n",
    "    return math.cos(pos/10000**(2*i/d))\n",
    "\n",
    "def Positional(t):\n",
    "    for pos,block in enumerate(t):\n",
    "        for i,_ in enumerate(block):\n",
    "            a = function(pos,i,len(block))\n",
    "            block[i]+=a\n",
    "    return t\n",
    "\n",
    "a = torch.randn(6, 5)\n",
    "b = torch.tensor(a, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df713054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
